 
========================================= 
PROJECT TREE STRUCTURE 
========================================= 
 
Folder PATH listing for volume OS
Volume serial number is 5808-1D48
C:.
|   main.py
|   __init__.py
|   
+---agents
|       health_assistant.py
|       workflows.py
|       __init__.py
|       
+---api
|   |   claude_client.py
|   |   llm_factory.py
|   |   llm_interface.py
|   |   weather_client.py
|   |   __init__.py
|   |   
|   \---__pycache__
|           claude_client.cpython-313.pyc
|           llm_factory.cpython-313.pyc
|           llm_interface.cpython-313.pyc
|           __init__.cpython-313.pyc
|           
+---core
|       context_assembly.py
|       conversation_orchestrator.py
|       response_generator.py
|       __init__.py
|       
+---memory
|   |   database.py
|   |   external_data.py
|   |   highlights.py
|   |   insights.py
|   |   knowledge.py
|   |   raw_data.py
|   |   __init__.py
|   |   
|   \---__pycache__
|           database.cpython-313-pytest-8.4.1.pyc
|           database.cpython-313.pyc
|           __init__.cpython-313.pyc
|           
+---utils
|   |   config.py
|   |   mock_data.py
|   |   __init__.py
|   |   
|   \---__pycache__
|           logging.cpython-313.pyc
|           
\---__pycache__
        __init__.cpython-313.pyc
        
 
 
========================================= 
FILE CONTENTS 
========================================= 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\main.py 
----------------------------------------- 
# Entry point for POC demo 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\__init__.py 
----------------------------------------- 
 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\agents\health_assistant.py 
----------------------------------------- 
# Main conversation agent 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\agents\workflows.py 
----------------------------------------- 
# LangGraph workflow definitions 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\agents\__init__.py 
----------------------------------------- 
 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\api\claude_client.py 
----------------------------------------- 
"""
Claude API Client - Implementation of LLM interface for Anthropic's Claude

Handles all Claude-specific API calls and converts them to our standard interface.
"""

import os
from typing import List, Dict, Optional
import anthropic
from anthropic import APIError, RateLimitError, APIConnectionError

from .llm_interface import LLMClient, LLMError, LLMUnavailableError, LLMRateLimitError


class ClaudeClient(LLMClient):
    """Claude implementation of the LLM interface"""

    def __init__(self, api_key: Optional[str] = None, model: str = "claude-sonnet-4-20250514"):
        """
        Initialize Claude client

        Args:
            api_key: Anthropic API key (defaults to ANTHROPIC_API_KEY env var)
            model: Claude model to use
        """
        self.api_key = api_key or os.getenv("ANTHROPIC_API_KEY")
        self.model = model

        if not self.api_key:
            raise LLMError("No API key provided. Set ANTHROPIC_API_KEY environment variable.", "claude")

        self.client = anthropic.Anthropic(api_key=self.api_key)

    def chat(
        self,
        user_message: str,
        conversation_history: Optional[List[Dict[str, str]]] = None,
        system_prompt: Optional[str] = None,
        **kwargs
    ) -> str:
        """
        Send a message to Claude and get response

        Args:
            user_message: The user's current message
            conversation_history: Previous conversation messages
            system_prompt: System prompt to set context
            **kwargs: Claude-specific parameters (temperature, max_tokens, etc.)

        Returns:
            str: Claude's response
        """
        try:
            # Build messages array
            messages = []

            # Add conversation history if provided
            if conversation_history:
                messages.extend(conversation_history)

            # Add current user message
            messages.append({"role": "user", "content": user_message})

            # Prepare API call parameters
            api_params = {
                "model": self.model,
                "max_tokens": kwargs.get("max_tokens", 1000),
                "messages": messages
            }

            # Add system prompt if provided
            if system_prompt:
                api_params["system"] = system_prompt

            # Add other Claude-specific parameters
            if "temperature" in kwargs:
                api_params["temperature"] = kwargs["temperature"]

            # Make API call
            response = self.client.messages.create(**api_params)

            # Extract text from response
            if response.content and len(response.content) > 0:
                return response.content[0].text
            else:
                raise LLMError("Empty response from Claude", "claude")

        except RateLimitError as e:
            raise LLMRateLimitError("Claude rate limit exceeded", "claude", e)
        except APIConnectionError as e:
            raise LLMUnavailableError("Cannot connect to Claude API", "claude", e)
        except APIError as e:
            raise LLMError(f"Claude API error: {str(e)}", "claude", e)
        except Exception as e:
            raise LLMError(f"Unexpected error with Claude: {str(e)}", "claude", e)

    def is_available(self) -> bool:
        """
        Test if Claude API is accessible

        Returns:
            bool: True if available, False otherwise
        """
        try:
            # Send a minimal test message
            test_response = self.client.messages.create(
                model=self.model,
                max_tokens=10,
                messages=[{"role": "user", "content": "test"}]
            )
            return True
        except Exception:
            return False

    def __str__(self):
        return f"ClaudeClient(model={self.model})" 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\api\llm_factory.py 
----------------------------------------- 
"""
LLM Factory - Creates LLM clients with fallback logic

Handles provider selection, fallback chains, and client configuration.
"""

import os
from typing import Optional, List
from dotenv import load_dotenv

from .llm_interface import LLMClient, LLMError
from .claude_client import ClaudeClient

# Load environment variables
load_dotenv()


class LLMFactory:
    """Factory for creating LLM clients with fallback support"""

    @staticmethod
    def create_client(provider: str = "claude", **kwargs) -> LLMClient:
        """
        Create a single LLM client

        Args:
            provider: LLM provider name ("claude", "openai", etc.)
            **kwargs: Provider-specific configuration

        Returns:
            LLMClient: Configured LLM client

        Raises:
            LLMError: If provider is unknown or can't be created
        """
        provider = provider.lower()

        if provider == "claude":
            return ClaudeClient(**kwargs)
        # elif provider == "openai":
        #     return OpenAIClient(**kwargs)  # Future implementation
        # elif provider == "local":
        #     return LocalLLMClient(**kwargs)  # Future implementation
        else:
            raise LLMError(f"Unknown LLM provider: {provider}")

    @staticmethod
    def get_default_client() -> LLMClient:
        """
        Get the default LLM client based on environment configuration

        Returns:
            LLMClient: Default configured client
        """
        # Check environment for preferred provider
        preferred_provider = os.getenv("LLM_PROVIDER", "claude").lower()

        try:
            return LLMFactory.create_client(preferred_provider)
        except LLMError:
            # Fallback to Claude if preferred provider fails
            return LLMFactory.create_client("claude")


    @staticmethod
    def create_client_with_fallback(
        primary_provider: str = "claude",
        fallback_providers: Optional[List[str]] = None
    ) -> LLMClient:
        """
        Create LLM client with built-in fallback logic

        Args:
            primary_provider: Primary provider to try first
            fallback_providers: List of fallback providers

        Returns:
            LLMClient: Client that handles fallback automatically
        """
        providers_to_try = [primary_provider] + (fallback_providers or [])

        for provider in providers_to_try:
            try:
                client = LLMFactory.create_client(provider)
                if client.is_available():
                    return client
                else:
                    print(f"Provider {provider} not available, trying next...")
            except LLMError as e:
                print(f"Failed to create {provider} client: {e.message}")
                continue

        # If all fail, return the primary anyway and let it handle errors
        return LLMFactory.create_client(primary_provider)


# Convenience functions for easy usage
def get_llm_client(with_fallback: bool = True) -> LLMClient:
    """
    Get an LLM client (simple interface for the conversation system)

    Args:
        with_fallback: Whether to use fallback providers

    Returns:
        LLMClient: Ready-to-use LLM client
    """
    if with_fallback:
        # For POC, just Claude since it's the only one we have
        # In the future: add fallback_providers=["openai", "local"]
        return LLMFactory.create_client_with_fallback("claude", [])
    else:
        return LLMFactory.get_default_client()


def test_all_providers() -> dict:
    """
    Test all available providers and return status

    Returns:
        dict: Provider status information
    """
    results = {}

    # Test Claude
    try:
        claude = LLMFactory.create_client("claude")
        results["claude"] = {
            "available": claude.is_available(),
            "error": None
        }
    except Exception as e:
        results["claude"] = {
            "available": False,
            "error": str(e)
        }

    # Add other providers here when implemented

    return results
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\api\llm_interface.py 
----------------------------------------- 
"""
LLM Interface - Abstract base class for all LLM providers

This interface provides a clean abstraction for different LLM providers
(Claude, OpenAI, local models, etc.) so the conversation system can
work with any provider without knowing implementation details.
"""

from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional


class LLMClient(ABC):
    """Abstract base class for LLM providers"""

    @abstractmethod
    def chat(
            self,
            user_message: str,
            conversation_history: Optional[List[Dict[str, str]]] = None,
            system_prompt: Optional[str] = None,
            **kwargs
    ) -> str:
        """
        Main conversational method

        Args:
            user_message: The user's current message
            conversation_history: List of previous messages in format:
                [{"role": "user", "content": "..."}, {"role": "assistant", "content": "..."}]
            system_prompt: Optional system prompt to override default
            **kwargs: Provider-specific parameters (temperature, max_tokens, etc.)

        Returns:
            str: The LLM's response message

        Raises:
            LLMError: If the API call fails
        """
        pass

    @abstractmethod
    def is_available(self) -> bool:
        """
        Quick health check to see if this provider is accessible

        Returns:
            bool: True if provider is available, False otherwise
        """
        pass


class LLMError(Exception):
    """Base exception for LLM-related errors"""

    def __init__(self, message: str, provider: str = None, original_error: Exception = None):
        self.message = message
        self.provider = provider
        self.original_error = original_error
        super().__init__(self.message)


class LLMUnavailableError(LLMError):
    """Raised when an LLM provider is temporarily unavailable"""
    pass


class LLMRateLimitError(LLMError):
    """Raised when rate limits are exceeded"""
    pass 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\api\weather_client.py 
----------------------------------------- 
# Weather API client 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\api\__init__.py 
----------------------------------------- 
 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\core\context_assembly.py 
----------------------------------------- 
# Context assembly 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\core\conversation_orchestrator.py 
----------------------------------------- 
"""
Conversation Orchestrator - LangGraph-based conversation management

Handles the main conversation flow using LangGraph state management and workflows.
"""

from typing import Dict, List, Optional, Any
from typing_extensions import TypedDict
from langgraph.graph import StateGraph, END

from ..api.llm_factory import get_llm_client
from ..api.llm_interface import LLMError


class ConversationState(TypedDict):
    """State structure for the conversation workflow"""
    user_id: str
    user_message: str
    conversation_history: List[Dict[str, str]]
    assembled_context: Dict[str, Any]
    system_prompt: str
    response: str
    error: Optional[str]


class ConversationOrchestrator:
    """
    Main conversation orchestrator using LangGraph workflows
    """

    def __init__(self):
        """Initialize the conversation orchestrator"""
        self.llm_client = get_llm_client()
        self.workflow = self._build_workflow()

        # Base system prompt for the health assistant
        self.base_system_prompt = """You are a helpful and encouraging health assistant for Fitbit users. 

Your role:
- Provide personalized insights based on user's health data
- Offer actionable advice and gentle encouragement
- Keep responses conversational and supportive
- Reference specific data patterns when relevant
- Suggest follow-up actions or questions when appropriate

Guidelines:
- Always be positive and motivational
- Use specific data points to make insights concrete
- Avoid medical diagnosis - focus on lifestyle and wellness
- Keep responses concise but helpful
- Ask follow-up questions to keep the conversation engaging"""

    def _build_workflow(self) -> Any:
        """Build the LangGraph conversation workflow"""

        # Define the workflow graph
        workflow = StateGraph(ConversationState)

        # Add nodes
        workflow.add_node("load_context", self._load_context)
        workflow.add_node("build_prompt", self._build_system_prompt)
        workflow.add_node("generate_response", self._generate_response)
        workflow.add_node("update_memory", self._update_memory)

        # Define the flow
        workflow.set_entry_point("load_context")
        workflow.add_edge("load_context", "build_prompt")
        workflow.add_edge("build_prompt", "generate_response")
        workflow.add_edge("generate_response", "update_memory")
        return workflow.compile()

    def _load_context(self, state: ConversationState) -> ConversationState:
        """
        Load and assemble context from all 6 memory layers

        For now, this uses mock data. Will be replaced with real memory system.
        """
        user_id = state["user_id"]

        # TODO: Replace with real context assembly
        mock_context = {
            "raw_data": {
                "recent_steps": [8500, 12000, 6800, 9200, 10500],
                "sleep_hours": [7.2, 6.8, 8.1, 7.5, 6.9],
                "resting_hr": [68, 70, 67, 69, 71],
                "user_profile": {
                    "age": 32,
                    "goals": ["better_sleep", "10k_steps_daily"],
                    "location": "Tel Aviv"
                }
            },
            "insights": [
                "Sleep quality improving over past week",
                "Step count varies significantly on weekends",
                "Resting heart rate stable and healthy"
            ],
            "highlights": {
                "preferences": "Prefers specific workout suggestions",
                "concerns": "Mentioned work stress affecting sleep",
                "tried_interventions": "Started evening breathing exercises"
            },
            "external_data": {
                "weather": "Sunny, 24°C in Tel Aviv",
                "air_quality": "Good"
            }
        }

        state["assembled_context"] = mock_context
        return state

    def _build_system_prompt(self, state: ConversationState) -> ConversationState:
        """
        Build the complete system prompt with context
        """
        context = state["assembled_context"]

        # Format context sections
        health_data_section = self._format_health_data(context.get("raw_data", {}))
        insights_section = self._format_insights(context.get("insights", []))
        highlights_section = self._format_highlights(context.get("highlights", {}))
        external_section = self._format_external_data(context.get("external_data", {}))

        # Build complete system prompt
        full_prompt = f"""{self.base_system_prompt}

---
CURRENT HEALTH DATA:
{health_data_section}

RECENT INSIGHTS:
{insights_section}

USER CONTEXT:
{highlights_section}

EXTERNAL CONTEXT:
{external_section}

---
Remember to use this context to provide personalized, specific responses to the user's question."""

        state["system_prompt"] = full_prompt
        return state

    def _format_health_data(self, raw_data: Dict) -> str:
        """Format raw health data for the prompt"""
        if not raw_data:
            return "No recent health data available."

        formatted = []

        if "recent_steps" in raw_data:
            steps = raw_data["recent_steps"]
            avg_steps = sum(steps) / len(steps) if steps else 0
            formatted.append(f"Recent daily steps: {steps} (avg: {avg_steps:.0f})")

        if "sleep_hours" in raw_data:
            sleep = raw_data["sleep_hours"]
            avg_sleep = sum(sleep) / len(sleep) if sleep else 0
            formatted.append(f"Recent sleep duration: {sleep} hours (avg: {avg_sleep:.1f}h)")

        if "resting_hr" in raw_data:
            hr = raw_data["resting_hr"]
            avg_hr = sum(hr) / len(hr) if hr else 0
            formatted.append(f"Recent resting heart rate: {hr} bpm (avg: {avg_hr:.0f})")

        if "user_profile" in raw_data:
            profile = raw_data["user_profile"]
            formatted.append(f"User profile: Age {profile.get('age', 'unknown')}, Goals: {profile.get('goals', [])}")

        return "\n".join(formatted) if formatted else "No health data available."

    def _format_insights(self, insights: List[str]) -> str:
        """Format insights for the prompt"""
        if not insights:
            return "No recent insights available."

        return "\n".join([f"- {insight}" for insight in insights])

    def _format_highlights(self, highlights: Dict) -> str:
        """Format conversation highlights for the prompt"""
        if not highlights:
            return "No previous conversation context."

        formatted = []
        for key, value in highlights.items():
            formatted.append(f"{key.replace('_', ' ').title()}: {value}")

        return "\n".join(formatted) if formatted else "No conversation context."

    def _format_external_data(self, external: Dict) -> str:
        """Format external data for the prompt"""
        if not external:
            return "No external context available."

        formatted = []
        for key, value in external.items():
            formatted.append(f"{key.replace('_', ' ').title()}: {value}")

        return "\n".join(formatted) if formatted else "No external context."

    def _generate_response(self, state: ConversationState) -> ConversationState:
        """
        Generate response using the LLM
        """
        try:
            response = self.llm_client.chat(
                user_message=state["user_message"],
                conversation_history=state["conversation_history"],
                system_prompt=state["system_prompt"]
            )
            state["response"] = response
            state["error"] = None

        except LLMError as e:
            state["response"] = "I'm sorry, I'm having trouble processing your request right now. Please try again."
            state["error"] = str(e)

        return state

    def _update_memory(self, state: ConversationState) -> ConversationState:
        """
        Update conversation memory and extract highlights

        For now, this is a placeholder. Will be implemented with real memory system.
        """
        # TODO: Update conversation history in database
        # TODO: Extract highlights from conversation
        # TODO: Update user context

        # For now, just log that we're updating memory
        print(f"[Memory Update] User {state['user_id']}: {state['user_message'][:50]}...")

        return state

    def chat(self, user_id: str, user_message: str, conversation_history: Optional[List[Dict[str, str]]] = None) -> str:
        """
        Main entry point for conversation

        Args:
            user_id: Unique identifier for the user
            user_message: The user's current message
            conversation_history: Previous conversation messages

        Returns:
            str: The assistant's response
        """
        # Initialize state
        initial_state = ConversationState(
            user_id=user_id,
            user_message=user_message,
            conversation_history=conversation_history or [],
            assembled_context={},
            system_prompt="",
            response="",
            error=None
        )

        # Run the workflow
        final_state = self.workflow.invoke(initial_state)

        # Return the response
        return final_state["response"]

    def is_available(self) -> bool:
        """Check if the orchestrator is ready to handle conversations"""
        return self.llm_client.is_available()


# Convenience function for easy usage
def create_conversation_orchestrator() -> ConversationOrchestrator:
    """Create and return a conversation orchestrator instance"""
    return ConversationOrchestrator()
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\core\response_generator.py 
----------------------------------------- 
# Response generator 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\core\__init__.py 
----------------------------------------- 
 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\database.py 
----------------------------------------- 
"""
Database models and connection management

SQLAlchemy models for all database tables and database session management.
"""

import os
from datetime import datetime, timezone
from typing import Optional
from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Text, ForeignKey
# from sqlalchemy.ext.declarative import
from sqlalchemy.orm import sessionmaker, relationship, Session, declarative_base
from sqlalchemy.dialects.postgresql import UUID
from sqlalchemy import JSON
import uuid

from dotenv import load_dotenv

# Load environment variables
load_dotenv()

# Database configuration
DATABASE_URL = os.getenv("DATABASE_URL", "sqlite:///fitbit_ai_poc.db")

# SQLAlchemy setup
Base = declarative_base()
engine = create_engine(DATABASE_URL)
SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=engine)


class User(Base):
    """User profile and basic information"""
    __tablename__ = "users"

    id = Column(Integer, primary_key=True, index=True)
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    age = Column(Integer)
    gender = Column(String(10))
    location = Column(String(100))
    goals = Column(JSON)
    preferences = Column(JSON)

    # Relationships
    health_metrics = relationship("HealthMetric", back_populates="user")
    conversations = relationship("Conversation", back_populates="user")
    insights = relationship("Insight", back_populates="user")
    highlights = relationship("Highlight", back_populates="user")


class HealthMetric(Base):
    """Time-series health data"""
    __tablename__ = "health_metrics"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), index=True)
    metric_type = Column(String(50), index=True)
    value = Column(Float)
    timestamp = Column(DateTime, index=True)
    extra_data = Column(JSON)

    # Relationships
    user = relationship("User", back_populates="health_metrics")

    def __repr__(self):
        return f"<HealthMetric(user_id={self.user_id}, type={self.metric_type}, value={self.value})>"


class Conversation(Base):
    """Conversation sessions and message history"""
    __tablename__ = "conversations"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), index=True)
    session_id = Column(UUID(as_uuid=True), default=uuid.uuid4, index=True)
    messages = Column(JSON)
    status = Column(String(20), default="active")
    created_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    ended_at = Column(DateTime, nullable=True)

    # Relationships
    user = relationship("User", back_populates="conversations")
    highlights = relationship("Highlight", back_populates="conversation")

    def __repr__(self):
        return f"<Conversation(user_id={self.user_id}, status={self.status}, messages={len(self.messages or [])})>"


class Insight(Base):
    """Generated health insights and analysis"""
    __tablename__ = "insights"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), index=True)
    category = Column(String(50), index=True)
    finding = Column(Text)
    timeframe = Column(String(20))
    confidence = Column(Float)
    extra_data = Column(JSON)
    generated_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))
    expires_at = Column(DateTime)

    # Relationships
    user = relationship("User", back_populates="insights")

    def __repr__(self):
        return f"<Insight(user_id={self.user_id}, category={self.category}, confidence={self.confidence})>"


class Highlight(Base):
    """Extracted conversation highlights and user context"""
    __tablename__ = "highlights"

    id = Column(Integer, primary_key=True, index=True)
    user_id = Column(Integer, ForeignKey("users.id"), index=True)
    conversation_id = Column(Integer, ForeignKey("conversations.id"), index=True)
    structured_data = Column(JSON)
    unstructured_notes = Column(Text)
    extracted_at = Column(DateTime, default=lambda: datetime.now(timezone.utc))

    # Relationships
    user = relationship("User", back_populates="highlights")
    conversation = relationship("Conversation", back_populates="highlights")

    def __repr__(self):
        return f"<Highlight(user_id={self.user_id}, conversation_id={self.conversation_id})>"


class ExternalContext(Base):
    """External data like weather, air quality, etc."""
    __tablename__ = "external_context"

    id = Column(Integer, primary_key=True, index=True)
    context_type = Column(String(50), index=True)
    location = Column(String(100), index=True)
    data = Column(JSON)
    timestamp = Column(DateTime, default=lambda: datetime.now(timezone.utc), index=True)

    def __repr__(self):
        return f"<ExternalContext(type={self.context_type}, location={self.location})>"


class KnowledgeBase(Base):
    """Health knowledge base for advanced insights"""
    __tablename__ = "knowledge_base"

    id = Column(Integer, primary_key=True, index=True)
    topic = Column(String(100), index=True)
    content = Column(Text)
    source = Column(String(200))
    last_updated = Column(DateTime, default=lambda: datetime.now(timezone.utc))

    def __repr__(self):
        return f"<KnowledgeBase(topic={self.topic}, source={self.source})>"


# Database connection and session management
class DatabaseManager:
    """Handles database connections and session management"""

    def __init__(self, database_url: Optional[str] = None):
        self.database_url = database_url or DATABASE_URL
        self.engine = create_engine(self.database_url)
        self.SessionLocal = sessionmaker(autocommit=False, autoflush=False, bind=self.engine)

    def create_tables(self):
        """Create all database tables"""
        Base.metadata.create_all(bind=self.engine)

    def drop_tables(self):
        """Drop all database tables (use with caution!)"""
        Base.metadata.drop_all(bind=self.engine)

    def get_session(self) -> Session:
        """Get a database session"""
        return self.SessionLocal()

    def health_check(self) -> bool:
        """Check if database is accessible"""
        try:
            session = self.get_session()
            session.execute("SELECT 1")
            session.close()
            return True
        except Exception:
            return False


# Convenience functions
def get_db_session() -> Session:
    """Get a database session (convenience function)"""
    db_manager = DatabaseManager()
    return db_manager.get_session()


def init_database():
    """Initialize database tables"""
    db_manager = DatabaseManager()
    db_manager.create_tables()
    print("Database tables created successfully!")
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\external_data.py 
----------------------------------------- 
# External data (weather, etc.) 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\highlights.py 
----------------------------------------- 
# Conversation highlights 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\insights.py 
----------------------------------------- 
# Insights generation 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\knowledge.py 
----------------------------------------- 
# Knowledge base 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\raw_data.py 
----------------------------------------- 
# Raw data layer 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\memory\__init__.py 
----------------------------------------- 
 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\utils\config.py 
----------------------------------------- 
# Configuration management 
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\utils\mock_data.py 
----------------------------------------- 
"""
Mock Data Generator - Creates realistic health data for testing

Generates sample users, health metrics, conversations, and insights
to test the Fitbit AI system with realistic data patterns.
"""

import random
import sys
import os
from datetime import datetime, timedelta, timezone
from typing import List, Dict, Any
import uuid

# Add src to path for imports
sys.path.append('src')

from memory.database import (
    DatabaseManager, User, HealthMetric, Conversation,
    Insight, Highlight, ExternalContext, KnowledgeBase
)


class MockDataGenerator:
    """Generates realistic mock data for testing"""

    def __init__(self):
        self.db_manager = DatabaseManager()

        # Sample data patterns
        self.sample_names = ["Alice", "Bob", "Carol", "David", "Emma", "Frank", "Grace", "Henry"]
        self.sample_locations = ["Tel Aviv", "New York", "London", "San Francisco", "Toronto", "Sydney"]
        self.sample_goals = [
            ["lose_weight", "10k_steps_daily"],
            ["better_sleep", "reduce_stress"],
            ["marathon_training", "improve_endurance"],
            ["general_fitness", "consistent_exercise"],
            ["heart_health", "lower_resting_hr"],
            ["better_sleep", "morning_workouts"]
        ]

    def generate_sample_users(self, count: int = 5) -> List[int]:
        """Generate sample users with varied profiles"""
        user_ids = []
        session = self.db_manager.get_session()

        try:
            for i in range(count):
                user = User(
                    age=random.randint(22, 65),
                    gender=random.choice(["M", "F", "Other"]),
                    location=random.choice(self.sample_locations),
                    goals=random.choice(self.sample_goals),
                    preferences={
                        "communication_style": random.choice(["encouraging", "analytical", "casual"]),
                        "data_detail_level": random.choice(["high", "medium", "low"]),
                        "notification_frequency": random.choice(["daily", "weekly", "as_needed"]),
                        "preferred_exercise": random.choice(["walking", "running", "cycling", "gym", "yoga"])
                    }
                )
                session.add(user)
                session.flush()  # This assigns the ID without committing
                user_ids.append(user.id)  # Get the ID while session is active

            session.commit()
            print(f"✓ Created {count} sample users")

        except Exception as e:
            session.rollback()
            print(f"✗ Error creating users: {e}")
        finally:
            session.close()

        return user_ids

    def generate_health_metrics(self, user_id: int, days_back: int = 30):
        """Generate realistic health metrics for a user over time"""
        session = self.db_manager.get_session()

        try:
            # Base patterns for this user (individual variation)
            base_steps = random.randint(6000, 12000)
            base_resting_hr = random.randint(55, 75)
            base_sleep_hours = random.uniform(6.5, 8.5)

            for day in range(days_back):
                # Calculate date
                date = datetime.now(timezone.utc) - timedelta(days=day)

                # Day of week patterns
                is_weekend = date.weekday() >= 5

                # Generate daily steps (lower on weekends, some variation)
                daily_steps = base_steps
                if is_weekend:
                    daily_steps = int(daily_steps * random.uniform(0.7, 1.2))
                else:
                    daily_steps = int(daily_steps * random.uniform(0.8, 1.3))

                # Add daily step entry
                session.add(HealthMetric(
                    user_id=user_id,
                    metric_type="steps",
                    value=daily_steps,
                    timestamp=date.replace(hour=23, minute=59),
                    extra_data={"day_of_week": date.strftime("%A")}
                ))

                # Generate heart rate readings (3 per day: morning, afternoon, evening)
                for hour in [8, 14, 20]:
                    # Resting HR varies slightly
                    hr_variation = random.uniform(-5, 5)
                    # Higher HR in afternoon
                    if hour == 14:
                        hr_variation += random.uniform(5, 15)

                    hr_value = max(50, base_resting_hr + hr_variation)

                    session.add(HealthMetric(
                        user_id=user_id,
                        metric_type="heart_rate",
                        value=hr_value,
                        timestamp=date.replace(hour=hour, minute=random.randint(0, 59)),
                        extra_data={"reading_type": "resting" if hour in [8, 20] else "active"}
                    ))

                # Generate sleep data
                sleep_variation = random.uniform(-1.5, 1.5)
                sleep_hours = max(4, base_sleep_hours + sleep_variation)

                # Sleep quality score (0-100)
                sleep_quality = random.randint(60, 95)

                session.add(HealthMetric(
                    user_id=user_id,
                    metric_type="sleep_duration",
                    value=sleep_hours,
                    timestamp=date.replace(hour=7, minute=0),
                    extra_data={"quality_score": sleep_quality, "bedtime": "22:30"}
                ))

                # Occasionally add other metrics
                if random.random() < 0.3:  # 30% chance
                    session.add(HealthMetric(
                        user_id=user_id,
                        metric_type="active_minutes",
                        value=random.randint(10, 90),
                        timestamp=date.replace(hour=19, minute=0),
                        extra_data={"activity_type": random.choice(["walking", "running", "cycling"])}
                    ))

            session.commit()
            print(f"✓ Generated {days_back} days of health metrics for user {user_id}")

        except Exception as e:
            session.rollback()
            print(f"✗ Error generating health metrics: {e}")
        finally:
            session.close()

    def generate_insights(self, user_id: int):
        """Generate sample insights for a user"""
        session = self.db_manager.get_session()

        try:
            sample_insights = [
                {
                    "category": "trend",
                    "finding": "Your average daily steps have increased by 15% over the past two weeks",
                    "timeframe": "2_weeks",
                    "confidence": 0.85
                },
                {
                    "category": "pattern",
                    "finding": "You tend to sleep better on days when you get 8000+ steps",
                    "timeframe": "1_month",
                    "confidence": 0.72
                },
                {
                    "category": "goal_progress",
                    "finding": "You're 80% of the way to your weekly step goal",
                    "timeframe": "1_week",
                    "confidence": 0.95
                },
                {
                    "category": "anomaly",
                    "finding": "Your resting heart rate was unusually high yesterday morning",
                    "timeframe": "1_day",
                    "confidence": 0.68
                }
            ]

            for insight_data in sample_insights:
                insight = Insight(
                    user_id=user_id,
                    category=insight_data["category"],
                    finding=insight_data["finding"],
                    timeframe=insight_data["timeframe"],
                    confidence=insight_data["confidence"],
                    extra_data={
                        "metrics_analyzed": ["steps", "heart_rate", "sleep_duration"],
                        "algorithm": "trend_analysis_v1"
                    },
                    expires_at=datetime.now(timezone.utc) + timedelta(days=7)
                )
                session.add(insight)

            session.commit()
            print(f"✓ Generated {len(sample_insights)} insights for user {user_id}")

        except Exception as e:
            session.rollback()
            print(f"✗ Error generating insights: {e}")
        finally:
            session.close()

    def generate_conversation_history(self, user_id: int):
        """Generate sample conversation history"""
        session = self.db_manager.get_session()

        try:
            # Create a completed conversation
            past_conversation = Conversation(
                user_id=user_id,
                session_id=uuid.uuid4(),
                messages=[
                    {"role": "user", "content": "How did I sleep last night?", "timestamp": "2025-01-14T09:00:00Z"},
                    {"role": "assistant", "content": "You got 7.2 hours of sleep last night with a quality score of 78%. That's pretty good! You went to bed around 10:30 PM and had some restful deep sleep phases.", "timestamp": "2025-01-14T09:00:05Z"},
                    {"role": "user", "content": "What can I do to improve my sleep?", "timestamp": "2025-01-14T09:01:00Z"},
                    {"role": "assistant", "content": "Based on your data, I notice you sleep better on days when you get more steps. Try to get at least 8000 steps today, and consider doing some light stretching before bed.", "timestamp": "2025-01-14T09:01:08Z"}
                ],
                status="completed",
                ended_at=datetime.now(timezone.utc) - timedelta(days=1)
            )
            session.add(past_conversation)
            session.commit()

            # Create highlights from that conversation
            highlight = Highlight(
                user_id=user_id,
                conversation_id=past_conversation.id,
                structured_data={
                    "sleep_concerns": ["sleep_quality", "bedtime_routine"],
                    "interests": ["step_sleep_correlation"],
                    "preferred_advice_type": "specific_actionable"
                },
                unstructured_notes="User is motivated to improve sleep quality and interested in data correlations"
            )
            session.add(highlight)
            session.commit()

            print(f"✓ Generated conversation history for user {user_id}")

        except Exception as e:
            session.rollback()
            print(f"✗ Error generating conversation history: {e}")
        finally:
            session.close()

    def generate_external_context(self):
        """Generate sample external context data"""
        session = self.db_manager.get_session()

        try:
            # Weather data for different locations
            weather_data = [
                {
                    "context_type": "weather",
                    "location": "Tel Aviv",
                    "data": {"temperature": 24, "condition": "sunny", "humidity": 65, "air_quality": "good"}
                },
                {
                    "context_type": "weather",
                    "location": "New York",
                    "data": {"temperature": 8, "condition": "cloudy", "humidity": 70, "air_quality": "moderate"}
                }
            ]

            for data in weather_data:
                context = ExternalContext(
                    context_type=data["context_type"],
                    location=data["location"],
                    data=data["data"]
                )
                session.add(context)

            session.commit()
            print("✓ Generated external context data")

        except Exception as e:
            session.rollback()
            print(f"✗ Error generating external context: {e}")
        finally:
            session.close()

    def generate_all_mock_data(self, num_users: int = 3, days_back: int = 14):
        """Generate complete set of mock data"""
        print("=== GENERATING MOCK DATA ===")

        # Create tables if they don't exist
        self.db_manager.create_tables()

        # Generate users and get their IDs
        user_ids = self.generate_sample_users(num_users)

        # Generate data for each user using their IDs
        for user_id in user_ids:
            self.generate_health_metrics(user_id, days_back)
            self.generate_insights(user_id)
            self.generate_conversation_history(user_id)

        # Generate external context
        self.generate_external_context()

        print("=== MOCK DATA GENERATION COMPLETE ===")

        return user_ids


def main():
    """Generate mock data when run directly"""
    generator = MockDataGenerator()
    user_ids = generator.generate_all_mock_data(num_users=3, days_back=21)

    print(f"\nGenerated data for {len(user_ids)} users with IDs: {user_ids}")
    print("You can now test the system with realistic health data!")


if __name__ == "__main__":
    main()
 
 
 
----------------------------------------- 
FILE: C:\Users\dorgo\PycharmProjects\fitbit\src\utils\__init__.py 
----------------------------------------- 
 
 
 
